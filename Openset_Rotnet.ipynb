{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1635312103761,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "1dDmrBHVfqhX"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1635312105358,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "fhWsWZwlfr_u"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from models import resnet_rotnet\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1635312107691,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "h8-J2CFjftZY",
    "outputId": "d58f41c1-14ca-4ebd-d538-6b90d104808c"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1635312108752,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "o8HQpx82nmA_"
   },
   "outputs": [],
   "source": [
    "from datasets.osr_dataloader import MNIST_OSR, CIFAR10_OSR, CIFAR100_OSR, SVHN_OSR, Tiny_ImageNet_OSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"Center loss.\n",
    "    \n",
    "    Reference:\n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, feat_dim=512, use_gpu=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(\n",
    "        embeddings: torch.Tensor,  #  [B, E]\n",
    "    ):\n",
    "    B = embeddings.size(0)\n",
    "    dot_product = embeddings @ embeddings.T  # [B, B]\n",
    "    squared_norm = torch.diag(dot_product) # [B]\n",
    "    distances = squared_norm.view(1, B) - 2.0 * dot_product + squared_norm.view(B, 1)  # [B, B]\n",
    "    return torch.sqrt(nn.functional.relu(distances) + 1e-16)  # [B, B]\n",
    "  \n",
    "def get_positive_mask(\n",
    "        labels: torch.Tensor,  # [B]\n",
    "        device: torch.device\n",
    "    ):\n",
    "    B = labels.size(0)\n",
    "    labels_equal = labels.view(1, B) == labels.view(B, 1)  # [B, B]\n",
    "    indices_equal = torch.eye(B, dtype=torch.bool).cuda()  # [B, B]\n",
    "    return labels_equal & ~indices_equal  # [B, B]\n",
    "\n",
    "def get_negative_mask(\n",
    "        labels: torch.Tensor,  # [B]\n",
    "        device: torch.device\n",
    "    ):\n",
    "    B = labels.size(0)\n",
    "    labels_equal = labels.view(1, B) == labels.view(B, 1)  # [B, B]\n",
    "    indices_equal = torch.eye(B, dtype=torch.bool).cuda()  # [B, B]\n",
    "    return ~labels_equal & ~indices_equal  # [B, B]\n",
    "\n",
    "def get_triplet_mask(\n",
    "        labels: torch.Tensor,  # [B]\n",
    "        device: torch.device\n",
    "    ):\n",
    "\n",
    "    B = labels.size(0)\n",
    "\n",
    "    # Make sure that i != j != k\n",
    "    indices_equal = torch.eye(B, dtype=torch.bool).cuda()  # [B, B]\n",
    "    indices_not_equal = ~indices_equal  # [B, B]\n",
    "    i_not_equal_j = indices_not_equal.view(B, B, 1)  # [B, B, 1]\n",
    "    i_not_equal_k = indices_not_equal.view(B, 1, B)  # [B, 1, B]\n",
    "    j_not_equal_k = indices_not_equal.view(1, B, B)  # [1, B, B]\n",
    "    distinct_indices = i_not_equal_j & i_not_equal_k & j_not_equal_k  # [B, B, B]\n",
    "\n",
    "    # Make sure that labels[i] == labels[j] but labels[i] != labels[k]\n",
    "    labels_equal = labels.view(1, B) == labels.view(B, 1)  # [B, B]\n",
    "    i_equal_j = labels_equal.view(B, B, 1)  # [B, B, 1]\n",
    "    i_equal_k = labels_equal.view(B, 1, B)  # [B, 1, B]\n",
    "    valid_labels = i_equal_j & ~i_equal_k  # [B, B, B]\n",
    "\n",
    "    return distinct_indices & valid_labels  # [B, B, B]\n",
    "\n",
    "def test_get_distance_matrix(device_for_tests):\n",
    "    embeddings = torch.FloatTensor(\n",
    "        [[1, 1], \n",
    "        [7, 7], \n",
    "        [1, 1]], \n",
    "    ).to(device=device_for_tests)\n",
    "    distance_matrix = get_distance_matrix(embeddings)\n",
    "    assert torch.allclose(\n",
    "        torch.diag(distance_matrix), \n",
    "        torch.zeros(3, device=device_for_tests)\n",
    "    )\n",
    "    assert torch.allclose(distance_matrix, distance_matrix.T)\n",
    "    assert distance_matrix[0, 2] < distance_matrix[0, 1]\n",
    "\n",
    "\n",
    "def test_get_positive_mask(device_for_tests):\n",
    "    labels = torch.LongTensor([1, 2, 3, 1])\n",
    "    pos_mask = get_positive_mask(labels, device_for_tests)\n",
    "    assert pos_mask[0, 3]\n",
    "    assert not pos_mask[0, 1]\n",
    "    assert not pos_mask[0, 0] and not pos_mask[1, 1]\n",
    "\n",
    "\n",
    "def test_get_negative_mask(device_for_tests):\n",
    "    labels = torch.LongTensor([1, 2, 3, 1])\n",
    "    neg_mask = get_negative_mask(labels, device_for_tests)\n",
    "    assert not neg_mask[0, 3]\n",
    "    assert neg_mask[0, 1]\n",
    "    assert not neg_mask[0, 0] and not neg_mask[1, 1]\n",
    "\n",
    "\n",
    "def test_get_triplet_mask(device_for_tests):\n",
    "    labels = torch.LongTensor([1, 2, 3, 1, 3])\n",
    "    mask = get_triplet_mask(labels, device_for_tests)\n",
    "    assert mask[0, 3, 2]\n",
    "    assert mask[2, 4, 1]\n",
    "    assert mask[4, 2, 0]\n",
    "    assert not mask[0, 0, 0]\n",
    "    assert not mask[0, 3, 3]\n",
    "    assert not mask[0, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, resnet: nn.Module):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.embeddings = nn.Linear(512, 10).cuda()\n",
    "        \n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: torch.Tensor,  # [B, C, H, W]\n",
    "            labels: torch.Tensor  # [B]\n",
    "        ):\n",
    "        B = labels.size(0)\n",
    "        embeddings = self.embeddings(inputs)  # [B, E]\n",
    "        distance_matrix = get_distance_matrix(embeddings)  # [B, B]\n",
    "        with torch.no_grad():\n",
    "            mask_pos = get_positive_mask(labels, device)  # [B, B]\n",
    "            mask_neg = get_negative_mask(labels, device)  # [B, B]\n",
    "            triplet_mask = get_triplet_mask(labels, device)  # [B, B, B]\n",
    "            unmasked_triplets = torch.sum(triplet_mask)  # [1]\n",
    "            mu_pos = torch.mean(distance_matrix[mask_pos])  # [1]\n",
    "            mu_neg = torch.mean(distance_matrix[mask_neg])  # [1]\n",
    "            mu = mu_neg - mu_pos  # [1]\n",
    "        \n",
    "        distance_i_j = distance_matrix.view(B, B, 1)  # [B, B, 1]\n",
    "        distance_i_k = distance_matrix.view(B, 1, B)  # [B, 1, B]\n",
    "        triplet_loss_unmasked = distance_i_k - distance_i_j   # [B, B, B]\n",
    "        triplet_loss_unmasked = triplet_loss_unmasked[triplet_mask] # [valid_triplets]\n",
    "        hardest_triplets = triplet_loss_unmasked < max(mu, 0)  # [valid_triplets]\n",
    "        triplet_loss = triplet_loss_unmasked[hardest_triplets]  # [valid_triplets_after_mask]\n",
    "        triplet_loss = nn.functional.relu(triplet_loss)  # [valid_triplets_after_mask]\n",
    "\n",
    "        loss = triplet_loss.mean()\n",
    "        \"\"\"\n",
    "        logs = {\n",
    "            'positive_pairs': torch.sum(mask_pos).cpu().detach().item(),\n",
    "            'negative_pairs': torch.sum(mask_neg).cpu().detach().item(),\n",
    "            'mu_neg': mu_neg.cpu().detach().item(),\n",
    "            'mu_pos': mu_pos.cpu().detach().item(),\n",
    "            'valid_triplets': unmasked_triplets.cpu().detach().item(),\n",
    "            'valid_triplets_after_mask': triplet_loss.size(0),\n",
    "            'triplet_loss': triplet_loss.mean().cpu().detach().item()\n",
    "        }\n",
    "        \"\"\"\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1635312109545,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "BkrgPvj6hZYp"
   },
   "outputs": [],
   "source": [
    "def load_dataset(options):\n",
    "    print(\"{} Preparation\".format(options['dataset']))\n",
    "\n",
    "    if 'mnist' in options['dataset']:\n",
    "        Data = MNIST_OSR(known=options['known'], dataroot=options['dataroot'], batch_size=options['batch_size'],\n",
    "                         img_size=options['img_size'])\n",
    "        trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loader\n",
    "    elif 'cifar10' == options['dataset']:\n",
    "        Data = CIFAR10_OSR(known=options['known'], dataroot=options['dataroot'], batch_size=options['batch_size'],\n",
    "                           img_size=options['img_size'])\n",
    "        trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loader\n",
    "    elif 'svhn' in options['dataset']:\n",
    "        Data = SVHN_OSR(known=options['known'], dataroot=options['dataroot'], batch_size=options['batch_size'],\n",
    "                        img_size=options['img_size'])\n",
    "        trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loader\n",
    "    elif 'cifar100' in options['dataset']:\n",
    "        Data = CIFAR10_OSR(known=options['known'], dataroot=options['dataroot'], batch_size=options['batch_size'],\n",
    "                           img_size=options['img_size'])\n",
    "        trainloader, testloader = Data.train_loader, Data.test_loader\n",
    "        out_Data = CIFAR100_OSR(known=options['unknown'], dataroot=options['dataroot'],\n",
    "                                batch_size=options['batch_size'], img_size=options['img_size'])\n",
    "        outloader = out_Data.test_loader\n",
    "    else:\n",
    "        Data = Tiny_ImageNet_OSR(known=options['known'], dataroot=options['dataroot'], batch_size=options['batch_size'],\n",
    "                                 img_size=options['img_size'])\n",
    "        trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loader\n",
    "\n",
    "\n",
    "    return Data, trainloader, testloader, outloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1635312116739,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "MMIsj29fjeSA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# openset 탐지 능력을 검증하는 코드들 입니다.\n",
    "# ------------------------------------------------------------------------------\n",
    "def evaluate_openset(networks, dataloader_on, dataloader_off, **options):\n",
    "\n",
    "    # closed-set test-data에서 softmax-max값을 추출하여 저장합니다.\n",
    "    d_scores_on = get_openset_scores(dataloader_on, networks, open=False ,**options)\n",
    "\n",
    "    # open-set test-data에서 softmax-max값을 추출하여 저장합니다.\n",
    "    d_scores_off = get_openset_scores(dataloader_off, networks, open=True ,**options)\n",
    "\n",
    "\n",
    "    # closed-set을 클래스 '0' open-set을 클래스 '1'로 지정하여 label을 생성합니다.\n",
    "    y_true = np.array([0] * len(d_scores_on) + [1] * len(d_scores_off))\n",
    "\n",
    "    # 각 레이블당 confidence (softmax-max값)을 할당하여 저장합니다.\n",
    "    y_score = np.concatenate([d_scores_on, d_scores_off])\n",
    "\n",
    "    # 생성한 label값과 이에 해당하는 confidence값을 이용하여 AUROC값을 추출합니다.\n",
    "    auc_score = roc_auc_score(y_true, y_score)\n",
    "\n",
    "\n",
    "    #metrics.confusion_matrix(target_all, pred_all, labels=range(num_classes))\n",
    "\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1635312117723,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "qS3t27vijfu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_openset_scores(dataloader, networks,open, dataloader_train=None, **options):\n",
    "\n",
    "    #위 코드에서 사용되는 함수로 softmax의 max값을 추출하는 함수입니다.\n",
    "    openset_scores = openset_softmax_confidence(dataloader, networks,open=open)\n",
    "    return openset_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1635312120407,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "o6a9sYXbjgx1"
   },
   "outputs": [],
   "source": [
    "def openset_softmax_confidence(dataloader, netC, open=False):\n",
    "\n",
    "    # softmax의 max값을 추출하여 저장하는 부분입니다.\n",
    "\n",
    "    # 먼저 값을 저장할 list를 선언합니다.\n",
    "    openset_scores = []\n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "\n",
    "    #openset_prediction =print(metrics.confusion_matrix(target_all, pred_all, labels=range(num_classes)))\n",
    "\n",
    "    #dataloader를 통해서 data를 받으면서 softmax값을 추출하고 이의 max값을 저장해줍니다.\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "\n",
    "            #수정 전\n",
    "            # preds = F.softmax(netC(images), dim=1)\n",
    "\n",
    "            #수정 후\n",
    "            logits, features = netC(images)\n",
    "            preds = F.softmax(logits, dim=1)\n",
    "\n",
    "            pred_all.extend(preds.max(dim=1)[1].data.cpu().numpy())\n",
    "            target_all.extend(labels.data.cpu().numpy())\n",
    "\n",
    "            openset_scores.extend(preds.max(dim=1)[0].data.cpu().numpy())\n",
    "\n",
    "\n",
    "    # 마지막에 '-'를 붙여서 return하는 이유는 다음과 같습니다.\n",
    "    # 위에서 closed-set을 '0' 클래스, open-set을 '1' 클래스로 정의하였습니다.\n",
    "    # 이때 confidence값이 작으면 '0' 클래스, 크면 '1' 클래스로 지정되도록 현재 AUROC 계산함수는 인식합니다.\n",
    "    # 그러나 softmax-max output값은 closed-set ('0')이 큰 값을 가지고 open-set ('1')이 작은 값을 가집니다.\n",
    "    # 때문에 AUROC 함수가 인식하는 결과에 맞게 -를 붙여서 closed-set('0')이 작은 값, open-set ('1')은 큰값이 되도록 합니다.\n",
    "\n",
    "    # if open == True:\n",
    "    #     print(\"Open Confusion matrix\")\n",
    "    #     print(metrics.confusion_matrix(target_all, pred_all, labels=range(num_classes)))\n",
    "\n",
    "    return -np.array(openset_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1635312122702,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "AioFqODpjigQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1635312123640,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "Nf1plIssGSJF"
   },
   "outputs": [],
   "source": [
    "def create_Rot_batch(inputs):\n",
    "       num_trans=8\n",
    "\n",
    "       rot0 = inputs\n",
    "        rot90 = torch.rot90(inputs, 1, [2, 3])\n",
    "        rot180 = torch.rot90(inputs, 2, [2, 3])\n",
    "        rot270 = torch.rot90(inputs, 3, [2, 3])\n",
    "\n",
    "\n",
    "        '''\n",
    "        im90=rot90[0].cpu().numpy().transpose([1,2,0])\n",
    "        cv2.imwrite('./check_Rot_samples/90.png', np.clip(im90 * 255, 0, 255).astype(np.uint8))\n",
    "        im180 = rot180[0].cpu().numpy().transpose([1, 2, 0])\n",
    "        cv2.imwrite('./check_Rot_samples/180.png', np.clip(im180 * 255, 0, 255).astype(np.uint8))\n",
    "        im270 = rot270[0].cpu().numpy().transpose([1, 2, 0])\n",
    "        cv2.imwrite('./check_Rot_samples/270.png', np.clip(im270 * 255, 0, 255).astype(np.uint8))\n",
    "        '''\n",
    "\n",
    "        rot0_flip = torch.flip(rot0, [2])\n",
    "        rot90_flip = torch.flip(rot90, [2])\n",
    "        rot180_flip = torch.flip(rot180, [2])\n",
    "        rot270_flip = torch.flip(rot270, [2])\n",
    "\n",
    "        rot0_label = torch.zeros(inputs.size(0), 1)\n",
    "        rot90_label = rot0_label + 1\n",
    "        rot180_label = rot0_label + 2\n",
    "        rot270_label = rot0_label + 3\n",
    "        rot0_flip_label = rot0_label + 4\n",
    "        rot90_flip_label = rot0_label + 5\n",
    "        rot180_flip_label = rot0_label + 6\n",
    "        rot270_flip_label = rot0_label + 7\n",
    "\n",
    "        rot_data_cat = torch.stack((rot0, rot90, rot180, rot270, rot0_flip, rot90_flip, rot180_flip, rot270_flip),\n",
    "                                   dim=0)\n",
    "        rot_label_cat = torch.stack((rot0_label, rot90_label, rot180_label, rot270_label, rot0_flip_label,\n",
    "                                     rot90_flip_label, rot180_flip_label, rot270_flip_label), dim=0)\n",
    "\n",
    "        rot_data_cat = torch.transpose(rot_data_cat, 0, 1)\n",
    "        rot_label_cat = torch.transpose(rot_label_cat, 0, 1)\n",
    "\n",
    "        idx = torch.randint(num_trans, size=(rot_data_cat.size(0),))\n",
    "\n",
    "        sample_rot_data_batch = rot_data_cat[torch.arange(rot_data_cat.size(0)), idx]\n",
    "        sample_rot_label_batch = rot_label_cat[torch.arange(rot_label_cat.size(0)), idx]\n",
    "\n",
    "        return sample_rot_data_batch, sample_rot_label_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1635312800490,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "U8w5Iqe1jjmH"
   },
   "outputs": [],
   "source": [
    "def train(epoch,trainloader,f, CE_COEF, CEN_COEF, TRP_COEF):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(\"Current lr : {}\".format(get_lr(optimizer)))\n",
    "\n",
    "    f.write('\\nEpoch: %d \\n' % epoch)\n",
    "    f.write(\"Current lr : {} \\n\".format(get_lr(optimizer)))\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    cls_loss=0\n",
    "    rot_loss=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    features_list=[]\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        targets = targets.long()\n",
    "        inputs ,targets = inputs.to(device) ,targets.to(device)\n",
    "\n",
    "        Rot_inputs,Rot_labels =create_Rot_batch(inputs)\n",
    "\n",
    "        Rot_inputs, Rot_labels = Rot_inputs.to(device), Rot_labels.to(device)\n",
    "\n",
    "        Rot_labels = torch.reshape(Rot_labels, (-1,))\n",
    "        Rot_labels = Rot_labels.long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs,features = net(inputs)\n",
    "        Rot_outputs, Rot_features = net(Rot_inputs, Rot=True)\n",
    "\n",
    "\n",
    "        # print(net.linear.weight)\n",
    "        # print(net.linear.weight.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(features.shape)\n",
    "\n",
    "\n",
    "\n",
    "        c_loss = criterion(outputs, targets) * CE_COEF\n",
    "        c_loss += criterion_centerloss(features, targets) * CEN_COEF\n",
    "        c_loss += criterion_tripletloss(features, targets) * TRP_COEF\n",
    "        Rot_loss = criterion(Rot_outputs,Rot_labels )\n",
    "\n",
    "\n",
    "        total_loss = c_loss+Rot_loss*0.3\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cls_loss += c_loss\n",
    "        rot_loss += Rot_loss*0.3\n",
    "        train_loss += total_loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "      \n",
    "\n",
    "\n",
    "    print(\"Train result\")\n",
    "    print('Loss: %.3f , cls_loss: %.3f , rot_loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1),cls_loss/(batch_idx+1) ,rot_loss/(batch_idx+1) ,100.*correct/total, correct, total))\n",
    "    \n",
    "    f.write(\"Train result \\n\")\n",
    "    f.write('Loss: %.3f , cls_loss: %.3f , rot_loss: %.3f | Acc: %.3f%% (%d/%d) \\n' % (train_loss/(batch_idx+1),cls_loss/(batch_idx+1) ,rot_loss/(batch_idx+1), 100.*correct/total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1635312803992,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "eLueGxV_jlBH"
   },
   "outputs": [],
   "source": [
    "def test(epoch,testloader,f, CE_COEF, CEN_COEF, TRP_COEF):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            targets = targets.long()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs,features = net(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets) * CE_COEF\n",
    "            loss += criterion_centerloss(features, targets) * CEN_COEF\n",
    "            loss += criterion_tripletloss(features, targets) * TRP_COEF\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pred_all.extend(predicted.data.cpu().numpy())\n",
    "            target_all.extend(targets.data.cpu().numpy())\n",
    "\n",
    "    print(\"Test result\")\n",
    "    print('Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    f.write(\"Test result \\n\")\n",
    "    f.write('Loss: %.3f | Acc: %.3f%% (%d/%d) \\n' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1635312805455,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "QUrN7pi0x9iX",
    "outputId": "80150323-849c-4436-c986-5b725bdb20ed"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1635312806307,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "p1fOMMANxYSl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "splits = {\n",
    "    'mnist': [\n",
    "        [2, 4, 5, 9, 8, 3],\n",
    "        [3, 2, 6, 9, 4, 0],\n",
    "        [5, 8, 3, 2, 4, 6],\n",
    "        [3, 7, 8, 4, 0, 5],\n",
    "        [6, 3, 4, 9, 8, 2]\n",
    "    ],\n",
    "    'svhn': [\n",
    "        [5, 3, 7, 2, 8, 6],\n",
    "        [3, 8, 7, 6, 2, 5],\n",
    "        [8, 9, 4, 7, 2, 1],\n",
    "        [3, 8, 2, 5, 0, 6],\n",
    "        [4, 9, 2, 7, 1, 0]\n",
    "    ],\n",
    "    'cifar10': [\n",
    "        [0, 6, 4, 9, 1, 7],\n",
    "        [7, 6, 4, 9, 0, 1],\n",
    "        [1, 5, 7, 3, 9, 4],\n",
    "        [8, 6, 1, 9, 0, 7],\n",
    "        [2, 4, 1, 7, 9, 6]\n",
    "    ],\n",
    "    'cifar100': [\n",
    "        [4, 7, 9, 1],\n",
    "        [6, 7, 1, 9],\n",
    "        [9, 6, 1, 7],\n",
    "        [6, 4, 9, 1],\n",
    "        [1, 0, 9, 8]\n",
    "    ],\n",
    "    'cifar100-10': [\n",
    "        [30, 25, 1, 9, 8, 0, 46, 52, 49, 71],\n",
    "        [41, 9, 49, 40, 73, 60, 48, 30, 95, 71],\n",
    "        [8, 9, 49, 40, 73, 60, 48, 95, 30, 71],\n",
    "        [95, 60, 30, 73, 46, 49, 68, 99, 8, 71],\n",
    "        [33, 2, 3, 97, 46, 21, 64, 63, 88, 43]\n",
    "    ],\n",
    "    'cifar100-50': [\n",
    "        [27, 94, 29, 77, 88, 26, 69, 48, 75, 5, 59, 93, 39, 57, 45, 40, 78, 20, 98, 47, 66, 70, 91, 76, 41, 83, 99, 32, 53, 72, 2, 95, 21, 73, 84, 68, 35, 11, 55, 60, 30, 25, 1, 9, 8, 0, 46, 52, 49, 71],\n",
    "        [65, 97, 86, 24, 45, 67, 2, 3, 91, 98, 79, 29, 62, 82, 33, 76, 0, 35, 5, 16, 54, 11, 99, 52, 85, 1, 25, 66, 28, 84, 23, 56, 75, 46, 21, 72, 55, 68, 8, 69, 41, 9, 49, 40, 73, 60, 48, 30, 95, 71],\n",
    "        [20, 83, 65, 97, 94, 2, 93, 16, 67, 29, 62, 33, 24, 98, 5, 86, 35, 54, 0, 91, 52, 66, 85, 84, 56, 11, 1, 76, 25, 55, 21, 99, 72, 41, 23, 75, 28, 68, 69, 46, 8, 9, 49, 40, 73, 60, 48, 95, 30, 71],\n",
    "        [92, 82, 77, 64, 5, 33, 62, 56, 70, 0, 20, 28, 67, 14, 84, 53, 91, 29, 85, 2, 52, 83, 75, 35, 11, 21, 72, 98, 55, 1, 41, 76, 25, 66, 69, 9, 48, 54, 40, 23, 95, 60, 30, 73, 46, 49, 68, 99, 8, 71],\n",
    "        [47, 6, 19, 0, 62, 93, 59, 65, 54, 70, 34, 55, 23, 38, 72, 76, 53, 31, 78, 96, 77, 27, 92, 18, 82, 50, 98, 32, 1, 75, 83, 4, 51, 35, 80, 11, 74, 66, 36, 42, 33, 2, 3, 97, 46, 21, 64, 63, 88, 43]\n",
    "    ],\n",
    "    'cifar100-100': [\n",
    "        np.arange(100).tolist(),\n",
    "        np.arange(100).tolist(),\n",
    "        np.arange(100).tolist(),\n",
    "        np.arange(100).tolist(),\n",
    "        np.arange(100).tolist()\n",
    "    ],\n",
    "    'tiny_imagenet': [\n",
    "        [108, 147, 17, 58, 193, 123, 72, 144, 75, 167, 134, 14, 81, 171, 44, 197, 152, 66, 1, 133],\n",
    "        [198, 161, 91, 59, 57, 134, 61, 184, 90, 35, 29, 23, 199, 38, 133, 19, 186, 18, 85, 67],\n",
    "        [177, 0, 119, 26, 78, 80, 191, 46, 134, 92, 31, 152, 27, 60, 114, 50, 51, 133, 162, 93],\n",
    "        [98, 36, 158, 177, 189, 157, 170, 191, 82, 196, 138, 166, 43, 13, 152, 11, 75, 174, 193, 190],\n",
    "        [95, 6, 145, 153, 0, 143, 31, 23, 189, 81, 20, 21, 89, 26, 36, 170, 102, 177, 108, 169]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "ce = 100\n",
    "while ce >= 50:\n",
    "    cen = 100 - ce\n",
    "    trp = 0\n",
    "    while trp <= 100 - ce:\n",
    "        li.append([ce, cen, trp])\n",
    "        cen -= 10\n",
    "        trp += 10\n",
    "    ce -= 10\n",
    "li = np.asarray(li, dtype=float)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "executionInfo": {
     "elapsed": 205304,
     "status": "error",
     "timestamp": 1635313012872,
     "user": {
      "displayName": "Cheol-Ho Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFoxTj-yQF7ybiXqB8mz2W-CddzyvC1XRbgCObsw=s64",
      "userId": "17758400080030341502"
     },
     "user_tz": -540
    },
    "id": "b7SYsm_JjmjL",
    "outputId": "f9c79157-efeb-4534-a458-d0e83ee65626",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "if __name__=='__main__':\n",
    "    #실제 코드 실행하는 부분입니다.\n",
    "    options={}\n",
    "\n",
    "    options['dataset']='cifar10'\n",
    "    options['dataroot']='./data'\n",
    "    options['batch_size']=64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    img_size = 32\n",
    "    \n",
    "    for ce_coef, cen_coef, trp_coef in li:\n",
    "        print('==> Building model..')\n",
    "\n",
    "        num_classes = 10\n",
    "        net = resnet_rotnet.ResNet18()\n",
    "        #net.fc =nn.Linear(512,num_classes)\n",
    "        net = net.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion_centerloss = CenterLoss()\n",
    "        criterion_tripletloss = TripletLoss(net)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "        f1 = \"{:.2f}\".format(ce_coef)\n",
    "        f2 = \"{:.2f}\".format(cen_coef)\n",
    "        f3 = \"{:.2f}\".format(trp_coef)\n",
    "        AUROC_list=[]\n",
    "\n",
    "        with open(\"PATH/log/RotNet_{}_{}_{}_{}.txt\".format(options['dataset'], f1, f2, f3), 'w') as f:\n",
    "            print(\"ce_coef :{:.2f}\".format(ce_coef))\n",
    "            print(\"cen_coef :{:.2f}\".format(cen_coef))\n",
    "            print(\"trp_coef :{:.2f}\".format(trp_coef))\n",
    "            print()\n",
    "            f.write(\"ce_coef :{:.2f}\\n\".format(ce_coef))\n",
    "            f.write(\"cen_coef :{:.2f}\\n\".format(cen_coef))\n",
    "            f.write(\"trp_coef :{:.2f}\\n\".format(trp_coef))\n",
    "            f.write(\"\\n\")\n",
    "            for i in range(len(splits[options['dataset']])):\n",
    "                known = splits[options['dataset']][len(splits[options['dataset']]) - i - 1]\n",
    "                unknown = list(set(list(range(0, 10))) - set(known))\n",
    "\n",
    "                options.update(\n",
    "                    {\n",
    "                        'item': i,\n",
    "                        'known': known,\n",
    "                        'unknown': unknown,\n",
    "                        'img_size': img_size\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                Data, trainloader, testloader, outloader = load_dataset(options)\n",
    "\n",
    "                for epoch in range(start_epoch, start_epoch+EPOCH):\n",
    "                    print(\"dataset :{} , split: {}\".format(options['dataset'], i))\n",
    "                    f.write(\"dataset :{} , split: {} \\n\".format(options['dataset'], i))\n",
    "                    train(epoch,trainloader,f, ce_coef, cen_coef, trp_coef) #train 함수 호출\n",
    "                    test(epoch,testloader,f, ce_coef, cen_coef, trp_coef)  #test 함수 호출\n",
    "\n",
    "                    # 앞서 보았던 evaludate_openset함수를 실행하고 output인 auroc값을 출력\n",
    "                    # 이때 입력으로는 network, closed-testloader, open-testloader를 줌.\n",
    "\n",
    "                    cur_auroc=evaluate_openset(net,testloader,outloader)\n",
    "\n",
    "                    print(\"AUROC : {:.2f} \".format(cur_auroc))\n",
    "                    print(\"\")\n",
    "                    f.write(\"AUROC : {:.2f} \\n\\n\".format(cur_auroc))\n",
    "\n",
    "\n",
    "                    scheduler.step()\n",
    "                AUROC_list.append(cur_auroc)\n",
    "\n",
    "            AUROC=np.asarray(AUROC_list)\n",
    "            print(f\"AUROC list : {AUROC}\")\n",
    "            print(\"split mean AUROC :{:.2f}\".format(np.mean(AUROC)))\n",
    "\n",
    "            f.write(f\"AUROC list : {AUROC}\\n\")\n",
    "            f.write(\"split mean AUROC :{:.2f}\\n\".format(np.mean(AUROC)))\n",
    "            f.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPNxslKNQEEPEg5lCPKicA8",
   "collapsed_sections": [],
   "name": "Openset_baseline_Rotnet.ipynb",
   "provenance": [
    {
     "file_id": "1Hnzaxb8vWK4CWCDnof11vEiSumpS9hAg",
     "timestamp": 1635310475659
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
